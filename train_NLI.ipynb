{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "import time\n",
    "import argparse\n",
    "import sys, os\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "\n",
    "from data import get_nli, get_batch, build_vocab\n",
    "from mutils import get_optimizer, dotdict\n",
    "from models import NLINet\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "GLOVE_PATH = \"dataset/GloVe/glove.840B.300d.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loadDataset(filename, size=-1):\n",
    "    label_category = {\n",
    "        'neutral': 0,\n",
    "        'entailment': 1,\n",
    "        'contradiction': 2\n",
    "    }\n",
    "    dataset = []\n",
    "    sentence1 = []\n",
    "    sentence2 = []\n",
    "    labels = []\n",
    "    with open(filename, 'r') as f:\n",
    "        i = 0\n",
    "        not_found = 0\n",
    "        for line in f:\n",
    "            row = json.loads(line, 'utf-8')\n",
    "            if size == -1 or i < size:\n",
    "                dataset.append(row)\n",
    "                label = row['gold_label'].strip()\n",
    "                if label in label_category:\n",
    "                    sentence1.append( row['sentence1'].strip() )\n",
    "                    sentence2.append( row['sentence2'].strip() )\n",
    "\n",
    "                    labels.append( label_category[label] )\n",
    "                    i += 1\n",
    "                else:\n",
    "                    not_found += 1\n",
    "            else:\n",
    "                break;\n",
    "        if not_found > 0:\n",
    "            print('Label not recognized %d' % not_found)\n",
    "                \n",
    "    return (dataset, sentence1, sentence2, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label not recognized 185\n",
      "Label not recognized 168\n",
      "Label not recognized 9796\n",
      "Label not recognized 9847\n"
     ]
    }
   ],
   "source": [
    "(train_dataset, train_sentence1, train_sentence2, train_labels) = loadDataset('./multinli_all/multinli_0.9_train.jsonl')\n",
    "(dev_matched_dataset, dev_matched_sentence1, dev_matched_sentence2, dev_matched_labels) = loadDataset('./multinli_all/multinli_0.9_dev_matched.jsonl')\n",
    "(dev_mismatched_dataset, dev_mismatched_sentence1, dev_mismatched_sentence2, dev_mismatched_labels) = loadDataset('./multinli_all/multinli_0.9_dev_mismatched.jsonl')\n",
    "(test_matched_dataset, test_matched_sentence1, test_matched_sentence2, test_matched_labels) = loadDataset('./multinli_all/multinli_0.9_test_matched_unlabeled.jsonl')\n",
    "(test_mismatched_dataset, test_mismatched_sentence1, test_mismatched_sentence2, test_mismatched_labels) = loadDataset('./multinli_all/multinli_0.9_test_mismatched_unlabeled.jsonl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_set = pd.DataFrame(train_dataset)\n",
    "dev_matched_set = pd.DataFrame(dev_matched_dataset)\n",
    "dev_mismatched_set = pd.DataFrame(dev_mismatched_dataset)\n",
    "test_matched_set = pd.DataFrame(test_matched_dataset)\n",
    "test_mismatched_set = pd.DataFrame(test_mismatched_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dev_matched_set = dev_matched_set[dev_matched_set['gold_label']!='-']\n",
    "dev_mismatched_set = dev_mismatched_set[dev_mismatched_set['gold_label']!='-']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sent1 = train_set['sentence1'].apply(TreebankWordTokenizer().tokenize)\n",
    "train_sent2 = train_set['sentence2'].apply(TreebankWordTokenizer().tokenize)\n",
    "dev_m_sent1 = dev_matched_set['sentence1'].apply(TreebankWordTokenizer().tokenize)\n",
    "dev_m_sent2 = dev_matched_set['sentence2'].apply(TreebankWordTokenizer().tokenize)\n",
    "dev_mis_sent1 = dev_mismatched_set['sentence1'].apply(TreebankWordTokenizer().tokenize)\n",
    "dev_mis_sent2 = dev_mismatched_set['sentence2'].apply(TreebankWordTokenizer().tokenize)\n",
    "test_m_sent1 = test_matched_set['sentence1'].apply(TreebankWordTokenizer().tokenize)\n",
    "test_m_sent2 = test_matched_set['sentence2'].apply(TreebankWordTokenizer().tokenize)\n",
    "test_mis_sent1 = test_mismatched_set['sentence1'].apply(TreebankWordTokenizer().tokenize)\n",
    "test_mis_sent2 = test_mismatched_set['sentence2'].apply(TreebankWordTokenizer().tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_word_dict(sentences):\n",
    "    # create vocab of words\n",
    "    word_dict = {}\n",
    "    for sent in sentences:\n",
    "        for word in sent:\n",
    "            if word not in word_dict:\n",
    "                word_dict[word] = ''\n",
    "    word_dict['<s>'] = ''\n",
    "    word_dict['</s>'] = ''\n",
    "    word_dict['<p>'] = ''\n",
    "    return word_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_glove(word_dict, glove_path):\n",
    "    # create word_vec with glove vectors\n",
    "    word_vec = {}\n",
    "    with open(glove_path) as f:\n",
    "        for line in f:\n",
    "            word, vec = line.split(' ', 1)\n",
    "            if word in word_dict:\n",
    "                word_vec[word] = np.array(list(map(float, vec.split())))\n",
    "    print('Found {0}(/{1}) words with glove vectors'.format(len(word_vec), len(word_dict)))\n",
    "    return word_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_vocab(sentences, glove_path):\n",
    "    word_dict = get_word_dict(sentences)\n",
    "    word_vec = get_glove(word_dict, glove_path)\n",
    "    print('Vocab size : {0}'.format(len(word_vec)))\n",
    "    return word_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train1 = list(train_sent1)+list(train_sent2)\n",
    "dev1 = list(dev_m_sent1)+list(dev_m_sent2)+list(dev_mis_sent1)+list(dev_mis_sent2)\n",
    "test1 = list(test_m_sent1)+list(test_m_sent2)+list(test_mis_sent1)+list(test_mis_sent2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 92111(/109562) words with glove vectors\n",
      "Vocab size : 92111\n"
     ]
    }
   ],
   "source": [
    "word_vec = build_vocab(train1+dev1+test1, GLOVE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_set = {}\n",
    "dev_matched_set = {}\n",
    "dev_mismatched_set = {}\n",
    "test_matched_set = {}\n",
    "test_mismatched_set = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_set['sentence1']  =list(train_sent1   )            \n",
    "train_set['sentence2']  =list(  train_sent2  )\n",
    "dev_matched_set['sentence1']  =list(  dev_m_sent1  )\n",
    "dev_matched_set['sentence2']  =list(  dev_m_sent2  )\n",
    "dev_mismatched_set['sentence1'] =list(  dev_mis_sent1) \n",
    "dev_mismatched_set['sentence2'] =list(  dev_mis_sent2 )\n",
    "test_matched_set['sentence1'] =list(  test_m_sent1 )\n",
    "test_matched_set['sentence2'] =list(  test_m_sent2 )\n",
    "test_mismatched_set['sentence1'] =list(  test_mis_sent1) \n",
    "test_mismatched_set['sentence2']=list(  test_mis_sent2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 92111(/109562) words with glove vectors\n",
      "Vocab size : 92111\n"
     ]
    }
   ],
   "source": [
    "word_vec = build_vocab(train_set['sentence1'] +train_set['sentence2'] +dev_matched_set['sentence1'] + dev_matched_set['sentence2'] +dev_mismatched_set['sentence1'] +dev_mismatched_set['sentence2'] + test_matched_set['sentence1'] + test_matched_set['sentence2'] +test_mismatched_set['sentence1'] +test_mismatched_set['sentence2'], GLOVE_PATH )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "for split in ['sentence1', 'sentence2']:\n",
    "    for data_type in ['train_set', 'dev_matched_set', 'dev_mismatched_set', 'test_matched_set','test_mismatched_set']:\n",
    "        eval(data_type)[split] = np.array([['<s>'] + [word for word in sent if word in word_vec] +\\\n",
    "                                          ['</s>'] for sent in eval(data_type)[split]])        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set['label'] = np.array(train_labels)\n",
    "dev_matched_set['label'] = np.array(dev_matched_labels)\n",
    "dev_mismatched_set['label'] = np.array(dev_mismatched_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "GLOVE_PATH = \"dataset/GloVe/glove.840B.300d.txt\"\n",
    "\n",
    "\n",
    "parser = argparse.ArgumentParser(description='NLI training')\n",
    "# paths\n",
    "parser.add_argument(\"--nlipath\", type=str, default='dataset/MultiNLI/', help=\"NLI data path (SNLI or MultiNLI)\")\n",
    "parser.add_argument(\"--outputdir\", type=str, default='savedir3/', help=\"Output directory\")\n",
    "parser.add_argument(\"--outputmodelname\", type=str, default='model.pickle')\n",
    "\n",
    "\n",
    "# training\n",
    "parser.add_argument(\"--n_epochs\", type=int, default=20)\n",
    "parser.add_argument(\"--batch_size\", type=int, default=64)\n",
    "parser.add_argument(\"--dpout_model\", type=float, default=0., help=\"encoder dropout\")\n",
    "parser.add_argument(\"--dpout_fc\", type=float, default=0., help=\"classifier dropout\")\n",
    "parser.add_argument(\"--nonlinear_fc\", type=float, default=0, help=\"use nonlinearity in fc\")\n",
    "parser.add_argument(\"--optimizer\", type=str, default=\"sgd,lr=0.1\", help=\"adam or sgd,lr=0.1\")\n",
    "parser.add_argument(\"--lrshrink\", type=float, default=5, help=\"shrink factor for sgd\")\n",
    "parser.add_argument(\"--decay\", type=float, default=0.99, help=\"lr decay\")\n",
    "parser.add_argument(\"--minlr\", type=float, default=1e-5, help=\"minimum lr\")\n",
    "parser.add_argument(\"--max_norm\", type=float, default=5., help=\"max norm (grad clipping)\")\n",
    "\n",
    "#model\n",
    "parser.add_argument(\"--encoder_type\", type=str, default='BLSTMEncoder', help=\"see list of encoders\")\n",
    "parser.add_argument(\"--enc_lstm_dim\", type=int, default=2048, help=\"encoder nhid dimension\")\n",
    "parser.add_argument(\"--n_enc_layers\", type=int, default=1, help=\"encoder num layers\")\n",
    "parser.add_argument(\"--fc_dim\", type=int, default=512, help=\"nhid of fc layers\")\n",
    "parser.add_argument(\"--n_classes\", type=int, default=3, help=\"entailment/neutral/contradiction\")\n",
    "parser.add_argument(\"--pool_type\", type=str, default='max', help=\"max or mean\")\n",
    "\n",
    "# gpu\n",
    "parser.add_argument(\"--gpu_id\", type=int, default=0, help=\"GPU ID\")\n",
    "parser.add_argument(\"--seed\", type=int, default=1234, help=\"seed\")\n",
    "\n",
    "\n",
    "params, _ = parser.parse_known_args(\" \".split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(params.seed)\n",
    "torch.manual_seed(params.seed)\n",
    "torch.cuda.manual_seed(params.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLINet (\n",
      "  (encoder): BLSTMEncoder (\n",
      "    (enc_lstm): LSTM(300, 2048, bidirectional=True)\n",
      "  )\n",
      "  (classifier): Sequential (\n",
      "    (0): Linear (16384 -> 512)\n",
      "    (1): Linear (512 -> 512)\n",
      "    (2): Linear (512 -> 3)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "params.word_emb_dim = 300\n",
    "\"\"\"\n",
    "MODEL\n",
    "\"\"\"\n",
    "# model config\n",
    "config_nli_model = {\n",
    "    'n_words'        :  len(word_vec)          ,\n",
    "    'word_emb_dim'   :  params.word_emb_dim   ,\n",
    "    'enc_lstm_dim'   :  params.enc_lstm_dim   ,\n",
    "    'n_enc_layers'   :  params.n_enc_layers   ,\n",
    "    'dpout_model'    :  params.dpout_model    ,\n",
    "    'dpout_fc'       :  params.dpout_fc       ,\n",
    "    'fc_dim'         :  params.fc_dim         ,\n",
    "    'bsize'          :  params.batch_size     ,\n",
    "    'n_classes'      :  params.n_classes      ,\n",
    "    'pool_type'      :  params.pool_type      ,\n",
    "    'nonlinear_fc'   :  params.nonlinear_fc   ,\n",
    "    'encoder_type'   :  params.encoder_type   ,\n",
    "    'use_cuda'       :  True                  ,\n",
    "\n",
    "}\n",
    "\n",
    "# model\n",
    "encoder_types = ['BLSTMEncoder', 'BLSTMprojEncoder', 'BGRUlastEncoder', 'InnerAttentionMILAEncoder',\\\n",
    "                 'InnerAttentionYANGEncoder', 'InnerAttentionNAACLEncoder', 'ConvNetEncoder', 'LSTMEncoder']\n",
    "assert params.encoder_type in encoder_types, \"encoder_type must be in \" + str(encoder_types)\n",
    "nli_net = NLINet(config_nli_model)\n",
    "print(nli_net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# loss\n",
    "weight = torch.FloatTensor(params.n_classes).fill_(1)\n",
    "loss_fn = nn.CrossEntropyLoss(weight=weight)\n",
    "loss_fn.size_average = False\n",
    "\n",
    "# optimizer\n",
    "optim_fn, optim_params = get_optimizer(params.optimizer)\n",
    "optimizer = optim_fn(nli_net.parameters(), **optim_params)\n",
    "\n",
    "# cuda by default\n",
    "nli_net.cuda()\n",
    "loss_fn.cuda()\n",
    "#src_embeddings.cuda()\n",
    "\n",
    "\n",
    "    \n",
    "\"\"\"\n",
    "TRAIN\n",
    "\"\"\"\n",
    "#src_embeddings.volatile = True\n",
    "val_acc_best = -1e10\n",
    "adam_stop = False\n",
    "stop_training = False\n",
    "lr = optim_params['lr'] if 'sgd' in params.optimizer else None\n",
    "#index_pad =word2id['<p>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def trainepoch(epoch):\n",
    "    print('\\nTRAINING : Epoch ' + str(epoch))\n",
    "    nli_net.train()\n",
    "    all_costs   = []\n",
    "    logs        = []\n",
    "    words_count = 0\n",
    "    \n",
    "    last_time = time.time()\n",
    "    correct = 0.\n",
    "    # shuffle the data\n",
    "    permutation = np.random.permutation(len(train_set['sentence1']))\n",
    "\n",
    "    s1 = train_set['sentence1'][permutation]\n",
    "    s2 = train_set['sentence2'][permutation]\n",
    "    target = train_set['label'][permutation]\n",
    "    \n",
    "\n",
    "    optimizer.param_groups[0]['lr'] = optimizer.param_groups[0]['lr'] * params.decay if epoch>1\\\n",
    "                                    and 'sgd' in params.optimizer else optimizer.param_groups[0]['lr']\n",
    "    print('Learning rate : {0}'.format(optimizer.param_groups[0]['lr']))\n",
    "\n",
    "    for stidx in range(0, len(s1), params.batch_size):\n",
    "        # prepare batch\n",
    "        s1_batch, s1_len = get_batch(s1[stidx:stidx + params.batch_size], word_vec)\n",
    "        s2_batch, s2_len = get_batch(s2[stidx:stidx + params.batch_size], word_vec)\n",
    "        s1_batch, s2_batch = Variable(s1_batch.cuda()), Variable(s2_batch.cuda())\n",
    "        tgt_batch = Variable(torch.LongTensor(target[stidx:stidx + params.batch_size])).cuda()\n",
    "        k = s1_batch.size(1)  # actual batch size\n",
    "#         print(s1_batch, s1_len) \n",
    "        # model forward\n",
    "        output = nli_net((s1_batch, s1_len), (s2_batch, s2_len))\n",
    "\n",
    "        pred = output.data.max(1)[1]\n",
    "        correct += pred.long().eq(tgt_batch.data.long()).cpu().sum()\n",
    "        assert len(pred) == len(s1[stidx:stidx + params.batch_size])\n",
    "        \n",
    "        # loss\n",
    "        loss = loss_fn(output, tgt_batch)\n",
    "        all_costs.append(loss.data[0])\n",
    "        words_count += (s1_batch.nelement() + s2_batch.nelement()) / params.word_emb_dim\n",
    "\n",
    "        # backward\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        \n",
    "        # gradient clipping (off by default)\n",
    "        shrink_factor = 1\n",
    "        total_norm = 0\n",
    "        \n",
    "        for p in nli_net.parameters():\n",
    "            if p.requires_grad:\n",
    "                p.grad.data.div_(k)  # divide by the actual batch size\n",
    "                total_norm += p.grad.data.norm() ** 2\n",
    "        total_norm = np.sqrt(total_norm)\n",
    "        \n",
    "        if total_norm > params.max_norm:\n",
    "            shrink_factor = params.max_norm / total_norm\n",
    "        current_lr = optimizer.param_groups[0]['lr'] # current lr (no external \"lr\", for adam)\n",
    "        optimizer.param_groups[0]['lr'] = current_lr * shrink_factor # just for update\n",
    "        \n",
    "        # optimizer step\n",
    "        optimizer.step()\n",
    "        optimizer.param_groups[0]['lr'] = current_lr\n",
    "\n",
    "        if len(all_costs) == 100:\n",
    "            logs.append( '{0} ; loss {1} ; sentence/s {2} ; words/s {3} ; accuracy train : {4}'.format(\n",
    "                    stidx, round(np.mean(all_costs),2), int(len(all_costs) * params.batch_size / (time.time() - last_time)),\n",
    "                    int(words_count * 1.0 / (time.time() - last_time)), round(100.*correct/(stidx+k), 2) ))\n",
    "            print(logs[-1])\n",
    "            last_time = time.time()\n",
    "            words_count = 0\n",
    "            all_costs = []\n",
    "    train_acc = round(100 * correct/len(s1), 2)\n",
    "    print('results : epoch {0} ; mean accuracy train : {1}'.format(epoch, train_acc))\n",
    "    return train_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate(epoch, eval_type='valid', matched = True, final_eval=False):\n",
    "    nli_net.eval()\n",
    "    correct = 0.\n",
    "    global val_acc_best, lr, stop_training, adam_stop\n",
    "    \n",
    "    if eval_type == 'valid' and matched==True:\n",
    "        print('\\nVALIDATION, matched : Epoch {0}'.format(epoch))\n",
    "    else:\n",
    "        print('\\nVALIDATION, mismatched : Epoch {0}'.format(epoch))\n",
    "        \n",
    "        \n",
    "    if matched:\n",
    "        data_m_set = dev_matched_set\n",
    "    else:\n",
    "        data_m_set = dev_mismatched_set\n",
    "    \n",
    "    \n",
    "    s1    = data_m_set['sentence1']    \n",
    "    s2    = data_m_set['sentence2']    \n",
    "    target = data_m_set['label'] \n",
    "\n",
    "\n",
    "    for i in range(0, len(s1), params.batch_size):\n",
    "        # prepare batch\n",
    "        s1_batch, s1_len = get_batch(s1[i:i + params.batch_size], word_vec)\n",
    "        s2_batch, s2_len = get_batch(s2[i:i + params.batch_size], word_vec)\n",
    "        s1_batch, s2_batch = Variable(s1_batch.cuda()), Variable(s2_batch.cuda())\n",
    "        tgt_batch = Variable(torch.LongTensor(target[i:i + params.batch_size])).cuda()\n",
    "        k = s1_batch.size(1)  # actual batch size\n",
    "            \n",
    "            \n",
    "        # model forward\n",
    "        output = nli_net((s1_batch, s1_len), (s2_batch, s2_len))\n",
    "        \n",
    "        pred = output.data.max(1)[1]\n",
    "        correct += pred.long().eq(tgt_batch.data.long()).cpu().sum()\n",
    "        \n",
    "        \n",
    "    # save model\n",
    "    eval_acc  = round(100 * correct / len(s1),2)\n",
    "    if final_eval:\n",
    "        print('finalgrep : accuracy {0} : {1}'.format(eval_type, eval_acc))\n",
    "    else:\n",
    "        print('togrep : results : epoch {0} ; mean accuracy {1} : {2}'.format(epoch, eval_type, eval_acc))\n",
    "    \n",
    "    if eval_type == 'valid' and epoch <= params.n_epochs:\n",
    "        if eval_acc > val_acc_best:\n",
    "            print('saving model at epoch {0}'.format(epoch))\n",
    "            if not os.path.exists(params.outputdir):\n",
    "                os.makedirs(params.outputdir)\n",
    "            torch.save(nli_net, os.path.join(params.outputdir, params.outputmodelname))\n",
    "            val_acc_best = eval_acc\n",
    "        else:\n",
    "            if 'sgd' in params.optimizer:\n",
    "                optimizer.param_groups[0]['lr'] = optimizer.param_groups[0]['lr'] / params.lrshrink\n",
    "                print('Shrinking lr by : {0}. New lr = {1}'.format(params.lrshrink, optimizer.param_groups[0]['lr']))\n",
    "                if optimizer.param_groups[0]['lr'] < params.minlr:\n",
    "                    stop_training = True\n",
    "            if 'adam' in params.optimizer:\n",
    "                # early stopping (at 2nd decrease in accuracy)\n",
    "                stop_training = adam_stop\n",
    "                adam_stop = True\n",
    "    return eval_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TRAINING : Epoch 1\n",
      "Learning rate : 0.1\n",
      "6336 ; loss 70.31 ; sentence/s 64 ; words/s 7345 ; accuracy train : 34.28\n",
      "12736 ; loss 70.23 ; sentence/s 70 ; words/s 7432 ; accuracy train : 34.91\n",
      "19136 ; loss 70.15 ; sentence/s 68 ; words/s 7468 ; accuracy train : 35.14\n",
      "25536 ; loss 69.97 ; sentence/s 67 ; words/s 7552 ; accuracy train : 35.89\n",
      "31936 ; loss 69.81 ; sentence/s 71 ; words/s 7445 ; accuracy train : 36.56\n",
      "38336 ; loss 69.49 ; sentence/s 66 ; words/s 7532 ; accuracy train : 37.09\n",
      "44736 ; loss 68.93 ; sentence/s 67 ; words/s 7447 ; accuracy train : 37.81\n",
      "51136 ; loss 68.31 ; sentence/s 63 ; words/s 7598 ; accuracy train : 38.48\n",
      "57536 ; loss 67.14 ; sentence/s 66 ; words/s 7541 ; accuracy train : 39.38\n",
      "63936 ; loss 65.69 ; sentence/s 66 ; words/s 7546 ; accuracy train : 40.29\n",
      "70336 ; loss 63.87 ; sentence/s 68 ; words/s 7439 ; accuracy train : 41.24\n",
      "76736 ; loss 62.1 ; sentence/s 70 ; words/s 7368 ; accuracy train : 42.23\n",
      "83136 ; loss 60.56 ; sentence/s 65 ; words/s 7412 ; accuracy train : 43.14\n",
      "89536 ; loss 59.04 ; sentence/s 63 ; words/s 7526 ; accuracy train : 44.16\n",
      "95936 ; loss 57.39 ; sentence/s 66 ; words/s 7566 ; accuracy train : 45.13\n",
      "102336 ; loss 57.6 ; sentence/s 69 ; words/s 7491 ; accuracy train : 45.97\n",
      "108736 ; loss 56.15 ; sentence/s 64 ; words/s 7553 ; accuracy train : 46.77\n",
      "115136 ; loss 56.91 ; sentence/s 69 ; words/s 7500 ; accuracy train : 47.46\n",
      "121536 ; loss 56.12 ; sentence/s 67 ; words/s 7532 ; accuracy train : 48.1\n",
      "127936 ; loss 55.01 ; sentence/s 63 ; words/s 7730 ; accuracy train : 48.75\n",
      "134336 ; loss 55.78 ; sentence/s 69 ; words/s 7496 ; accuracy train : 49.34\n",
      "140736 ; loss 56.03 ; sentence/s 65 ; words/s 7603 ; accuracy train : 49.87\n",
      "147136 ; loss 54.69 ; sentence/s 69 ; words/s 7507 ; accuracy train : 50.36\n",
      "153536 ; loss 54.39 ; sentence/s 66 ; words/s 7443 ; accuracy train : 50.85\n",
      "159936 ; loss 54.51 ; sentence/s 66 ; words/s 7498 ; accuracy train : 51.29\n",
      "166336 ; loss 53.14 ; sentence/s 69 ; words/s 7492 ; accuracy train : 51.73\n",
      "172736 ; loss 53.6 ; sentence/s 70 ; words/s 7414 ; accuracy train : 52.16\n",
      "179136 ; loss 52.88 ; sentence/s 70 ; words/s 7404 ; accuracy train : 52.56\n",
      "185536 ; loss 53.7 ; sentence/s 68 ; words/s 7476 ; accuracy train : 52.91\n",
      "191936 ; loss 53.86 ; sentence/s 66 ; words/s 7436 ; accuracy train : 53.23\n",
      "198336 ; loss 53.93 ; sentence/s 66 ; words/s 7476 ; accuracy train : 53.52\n",
      "204736 ; loss 52.06 ; sentence/s 68 ; words/s 7459 ; accuracy train : 53.87\n",
      "211136 ; loss 53.55 ; sentence/s 69 ; words/s 7428 ; accuracy train : 54.15\n",
      "217536 ; loss 53.09 ; sentence/s 67 ; words/s 7437 ; accuracy train : 54.4\n",
      "223936 ; loss 53.21 ; sentence/s 66 ; words/s 7619 ; accuracy train : 54.64\n",
      "230336 ; loss 52.81 ; sentence/s 65 ; words/s 7468 ; accuracy train : 54.88\n",
      "236736 ; loss 52.77 ; sentence/s 68 ; words/s 7483 ; accuracy train : 55.1\n",
      "243136 ; loss 52.13 ; sentence/s 66 ; words/s 7666 ; accuracy train : 55.35\n",
      "249536 ; loss 51.7 ; sentence/s 68 ; words/s 7467 ; accuracy train : 55.59\n",
      "255936 ; loss 52.35 ; sentence/s 69 ; words/s 7501 ; accuracy train : 55.79\n",
      "262336 ; loss 51.73 ; sentence/s 63 ; words/s 7534 ; accuracy train : 56.01\n",
      "268736 ; loss 52.04 ; sentence/s 67 ; words/s 7518 ; accuracy train : 56.21\n",
      "275136 ; loss 51.09 ; sentence/s 65 ; words/s 7598 ; accuracy train : 56.42\n",
      "281536 ; loss 51.12 ; sentence/s 66 ; words/s 7512 ; accuracy train : 56.61\n",
      "287936 ; loss 51.37 ; sentence/s 67 ; words/s 7587 ; accuracy train : 56.78\n",
      "294336 ; loss 51.33 ; sentence/s 67 ; words/s 7582 ; accuracy train : 56.95\n",
      "300736 ; loss 51.22 ; sentence/s 65 ; words/s 7617 ; accuracy train : 57.11\n",
      "307136 ; loss 51.64 ; sentence/s 65 ; words/s 7458 ; accuracy train : 57.28\n",
      "313536 ; loss 50.8 ; sentence/s 67 ; words/s 7448 ; accuracy train : 57.43\n",
      "319936 ; loss 51.87 ; sentence/s 67 ; words/s 7523 ; accuracy train : 57.56\n",
      "326336 ; loss 51.73 ; sentence/s 69 ; words/s 7392 ; accuracy train : 57.69\n",
      "332736 ; loss 50.97 ; sentence/s 67 ; words/s 7538 ; accuracy train : 57.84\n",
      "339136 ; loss 51.19 ; sentence/s 69 ; words/s 7455 ; accuracy train : 57.97\n",
      "345536 ; loss 51.03 ; sentence/s 64 ; words/s 7645 ; accuracy train : 58.1\n",
      "351936 ; loss 50.66 ; sentence/s 68 ; words/s 7468 ; accuracy train : 58.23\n",
      "358336 ; loss 51.03 ; sentence/s 65 ; words/s 7548 ; accuracy train : 58.34\n",
      "364736 ; loss 51.06 ; sentence/s 72 ; words/s 7381 ; accuracy train : 58.45\n",
      "371136 ; loss 52.22 ; sentence/s 68 ; words/s 7491 ; accuracy train : 58.55\n",
      "377536 ; loss 50.87 ; sentence/s 66 ; words/s 7535 ; accuracy train : 58.66\n",
      "383936 ; loss 50.83 ; sentence/s 68 ; words/s 7384 ; accuracy train : 58.77\n",
      "390336 ; loss 51.2 ; sentence/s 66 ; words/s 7417 ; accuracy train : 58.87\n",
      "results : epoch 1 ; mean accuracy train : 58.92\n",
      "\n",
      "VALIDATION, matched : Epoch 1\n",
      "togrep : results : epoch 1 ; mean accuracy valid : 65.36\n",
      "saving model at epoch 1\n",
      "\n",
      "VALIDATION, mismatched : Epoch 1\n",
      "togrep : results : epoch 1 ; mean accuracy valid : 66.2\n",
      "saving model at epoch 1\n",
      "\n",
      "TRAINING : Epoch 2\n",
      "Learning rate : 0.099\n",
      "6336 ; loss 48.85 ; sentence/s 68 ; words/s 7495 ; accuracy train : 66.69\n",
      "12736 ; loss 49.89 ; sentence/s 67 ; words/s 7469 ; accuracy train : 66.29\n",
      "19136 ; loss 49.72 ; sentence/s 69 ; words/s 7461 ; accuracy train : 66.1\n",
      "25536 ; loss 49.53 ; sentence/s 68 ; words/s 7535 ; accuracy train : 66.38\n",
      "31936 ; loss 49.06 ; sentence/s 67 ; words/s 7511 ; accuracy train : 66.52\n",
      "38336 ; loss 48.84 ; sentence/s 65 ; words/s 7626 ; accuracy train : 66.58\n",
      "44736 ; loss 49.99 ; sentence/s 67 ; words/s 7383 ; accuracy train : 66.52\n",
      "51136 ; loss 49.84 ; sentence/s 67 ; words/s 7542 ; accuracy train : 66.48\n",
      "57536 ; loss 50.01 ; sentence/s 66 ; words/s 7602 ; accuracy train : 66.41\n",
      "63936 ; loss 49.29 ; sentence/s 63 ; words/s 7635 ; accuracy train : 66.52\n"
     ]
    }
   ],
   "source": [
    "epoch = 1\n",
    "while not stop_training and epoch <= params.n_epochs:\n",
    "    train_acc = trainepoch(epoch)\n",
    "    eval_acc = evaluate(epoch, eval_type='valid', matched = True)\n",
    "    eval_acc = evaluate(epoch, eval_type='valid', matched = False)\n",
    "    epoch+=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
